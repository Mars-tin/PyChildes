model:

  name: gpt2-base
  class_name: GPT2LMHeadModel

  transformer:
    d_head: 64
    n_head: 12
    n_layer: 12

  tokenizer:
    model_max_length: 512

training:

  # general settings
  use_tf32: true
  use_amp: true
  amp_dtype: 'bf16'
  torch_compile: false
  grad_accum_steps: 1
  grad_clip_norm: 1.0

  # data loader
  batch_size_per_gpu: 8
  num_workers: 4
  num_threads: 32
  prefetch_factor: 32

  # optimizer and scheduler
  #   if num_epoch = -1 then set num_training_steps = num_steps;
  #   else num_training_steps = num_epoch * len(data)
  num_epoch: 2
  num_steps: -1

  lr: 0.00005
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.05
  warmup_steps: 1000
  reset_lr: false
  reset_weight_decay: false

  # checkpoint
  checkpoint_every: 500
  checkpoint_dir: '../experiments/checkpoints/template/'

  # wandb
  api_key_path: './api_keys.yaml'
  wandb_project: 'trabank'
  wandb_exp_name: 'template'
  wandb_log_every: 50
  wandb_offline: false
